Please write notes here everyday
helloooo halua
hello tuii

Some Reasons why Values are missing in our dataset:
a) The data wasn't recorded at the time.
b) Data Corruption.
c) Privacy reasons.
d) Maybe the data doesn't exist.
For example:
If there is a column named 'Everyday dinner contents' and if you happen to miss dinner one day then the value will not be existing and not recorded in our ds.

## To find out how many values and where the values are missing in maximum in our dataset,
df.isnull().sum() is used.
This is the count of missing data in each of the columns in the given dataset.

#All the missing values:
1. df.isnull().sum()

#Any missing values:
1. df.isnull().values.any()
2. df.isnull().sum().sum()

##
df['cN'].value_counts() by default excludes the missing values and displays how many times a same name in the column is repeated.
This excludes the NaN values.
df['cN'].value_counts(dropna=False) also includes the number of NaN repeated in the dataset.

## Methods of dropping:
a) df.dropna(how='any') #This drops the rows containig any (one or some or all) missing (NaN or na etc) values.
Unless specified it performs on the default axis which is 0.
df.dropna(how='any').shape 
# This helps know how many rows and columns exist after dropping the missing values by comparing with the shape of the undropped dataset.
b) df.dropna(how='all').shape # This only drops the row or otherwise specified column containig all missing Values.
c) df.dropna(subset=['columnName1_withmissingValues', 'cN2'], how='any').shape
The above mentioned code drops the row only if one of the two mentioned columns has a missing value.
d) df.dropna(subset=['cN', 'columnName'], how='all').shape 
# This drops the values only if all of the mentioned columns have null values.
e) df.drop('cN', axis='columns' or axis=1, inplace=True)
NOTE: By default, it is inplace=False
#This means it doesn't change or implement this in the original df. 
f) df.dropna(axis='columns', how='all') # To drop the columns having all of the values as null.

## Methods of filling using fillna:
a) df['cN'].fillna(value='any other relevant value in our dataset-someNameof the value', inplace = True) 
inplace = True makes the changes in the undropped untouched dataframe as well. This change is not temporary.
If inplace=True was not included, that wouldn't make changes to the ds as by default, inplace = False
So either we would have to assign it to something like:
filled_dataset = df['cN'].fillna(value='any other relevant value in our dataset-someNameof the value')
or by using the inplace = True

NOTE: Filling or replacing the values can  be done by using the loc method.

median = df['cN'].median()

## While comparing data in two columns and finding out the relation in numbers,
df[df.cN == value_in_the_column].another_cN.value_counts() 
In the above code, the contents in the bracket gives the meaning of what the comparison is on the basis of.
The contents after the bracket generally gives the  idea of what are we intending to find out.

## To find out the same above relation or to display the value count in percentage,
df[df.cN == value_in_the_column].another_cN.value_counts(normalize=True) 

# The same things as done above can be done using another method: groupby
df.groupby('cN').another_cN.value_counts(normalize=True) or
df.groupby('cN').another_cN.value_counts(normalize=True).loc[:, 'rValue_in_the_column']

##To obtain the above results from the above lines of code in a dataframe structure: unstack()
df.groupby('cN').another_cN.value_counts(normalize=True).unstack() 

##Types of Missing Values
a) NA, NaN, "" : Standard Types
b) n/a, na, "--" : Non-standard Types
c) 12
...

#To change the non-standard null values into standard ones:
missing_values = ["n/a", "na", "--"]
df=pd.read_csv("xyz.csv", na_values=missing_values)

##Comparing Groups
The selection of the column name is case sensitive.

To select a particular value name in a particular column of a particular df, the following code is used:
df[df.cN=='value_name'].anothercN.value_counts()

##Examining Relationships

df.cN.value_counts(normalize=True)
df.cN.mean()
True has a value of 1 and False has a value of 0.
The code counts all the values with 1 and calculates their mean.
OR
Percentage of times, it is a one.

If we're doing value counts on a boolean column, often we get counts of 0s and 1s.
sum=number of 1s 
mean=percentage of 1s

df.groupby('cN1').cN2.mean()
-For each cN1, we want to get the mean of cN2 or on the basis of cN1.
-% of Trues for cN2 of each cN1.

